{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zsh2000/CS598-DL4H/blob/main/DL4H_Team_109.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Note for TA/grader:\n",
        "\n",
        "After the project draft deadline, I continue working on trying to install the required packages on Google Colab, but I still fail to install the required packages with specific versions on Google Colab. Therefore, in the final report I still use the same strategy to run the code on my own machines with NVIDIA 1080/2080 GPUs, and take a screenshot of my experimental results, then copy the screenshot images to the Google Colab below. To remind you of what problems I have met during the installation on Google Colab, I list them here again:\n",
        "\n",
        "*   The project requires specifically Python 3.7 and pytorch=1.9.1+cu102. However, I tried for a long time to install this specific pytorch version but failed. No matter how I changed the python version or install the older version of pytorch, the environment on Google Colab is always cuda 12.2 with pytorch 2.0.\n",
        "*   I cannot install GLIP and maskrcnn_benchmark on Google Colab, as they need to be manually built (e.g., using python setup.py build develop).\n",
        "\n",
        "<!-- Therefore, I cannot run the code on Google Colab. Instead, I will only show the corresponding code snippets regarding every part of the project as below in the Google Colab. The actual experiments are run on my own machines with NVIDIA 1080/2080 GPUs. I would take a screenshot of my experimental results, and then copy the screenshot images to the Google Colab below. Even building the environment on my own machines, it is still difficult for me to work on this project. (But finally I made it!) My major difficulities include:\n",
        "\n",
        "*   The given dataset in the official repository on GitHub is actually not incomplete. The validation data is missing. Therefore, I find the full datasets from another repository https://github.com/DengPingFan/PraNet.\n",
        "\n",
        "*   The maskrcnn_benchmark/layers folder in the official repository actually missed an important file \"_utils.py\". I previously thought that it is because my installed pytorch/maskrcnn_benchmark packages are in the wrong version. But finally I find out that adding the _utils.py back to the directory would fix this issue after some time and effort.\n",
        "\n",
        "*   When building the maskrcnn_benchmark package from source code, we should specify CUDA_HOME with the path of CUDA 10.2. Otherwise, the maskrcnn_benchmark cannot use GPU when running the code. -->"
      ],
      "metadata": {
        "id": "28CHmUNamZV7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Project Video:\n",
        "\n",
        "https://drive.google.com/file/d/185NfXDg4ptb8NLxwAG7FuMx0d0ER1hjO/view?usp=sharing"
      ],
      "metadata": {
        "id": "fIr7OITIMs7E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "\n",
        "*   Background of the problem\n",
        "  * what type of problem: This paper focuses on medical image understanding with pretrained vision language models. The medical image understanding tasks investigated in the paper are mainly referring object detection tasks.\n",
        "  \n",
        "  * what is the importance/meaning of solving the problem: Currently vision language models have shown to be extremely powerful in domains like natural images and other image domains, so they have large potential for benefiting the medical image domain.\n",
        "\n",
        "  * what is the difficulty of the problem: The medical image domain requires a very high level of expert knowledge, so it is very challenging to effectively transfer the pretrained knowledge in these vision language models to the medical image domain.\n",
        "\n",
        "  * the state of the art methods and effectiveness: The state-of-the-art solution is to use knowledge transfer techniques to transfer the knowledge from the pretrained vision language models to the specific medical image domain. However, since the medical image domain has much expert knowledge specific in the domain, these knowledge transfer techniques are not that effective.\n",
        "\n",
        "*   Paper explanation\n",
        "\n",
        "  * what did the paper propose: This paper proposes approaches of automatic generation of medical prompts for the pretrained vision language models.\n",
        "\n",
        "  * what is the innovations of the method: The authors conduct a comprehensive study on different strategies of generating the medical prompts, giving a rough guideline of what kinds of prompts are most beneficial for the task.\n",
        "\n",
        "  * how well the proposed method work (in its own metrics): The proposed method outperforms all existing methods in medical image understanding tasks like object detection.\n",
        "\n",
        "  * what is the contribution to the reasearch regime (referring the Background above, how important the paper is to the problem): This paper presents an effective way of generating medical prompts, and successfully utilizing the generated prompts to use the knowledge from pretrained vision language models for medical image understanding. It is the first of accomplishing this goal effectively."
      ],
      "metadata": {
        "id": "MQ0sNuMePBXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scope of Reproducibility:\n",
        "\n",
        "List hypotheses from the paper you will test and the corresponding experiments you will run.\n",
        "\n",
        "\n",
        "1.   Hypothesis 1: Using the Masked Language Model (MLM) technology, we can realize auto-prompt generation.\n",
        "2.   Hypothesis 2: Using the automatically generated prompts, the knowledge transferability from pretrained vision language model (GLIP used in this paper) to the target medical image domain can be enhanced."
      ],
      "metadata": {
        "id": "uygL9tTPSVHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methodology\n",
        "\n",
        "This methodology is the core of your project. It consists of run-able codes with necessary annotations to show the expeiment you executed for testing the hypotheses.\n",
        "\n",
        "The methodology at least contains two subsections **data** and **model** in your experiment."
      ],
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment\n",
        "The required packages are listed in the requirements.txt file which can be referred in the Github repo and also in the original project repo at https://github.com/MembrAI/MIU-VL. The challenging parts of installing the environment include installing the specific version of Pytorch and CUDA. Also, the maskrcnn_benchmark package in the original repo has the inconsistent naming issue as discussed in the project draft."
      ],
      "metadata": {
        "id": "oVhQ5VamGebg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Data\n",
        "Data includes raw data (MIMIC III tables), descriptive statistics (our homework questions), and data processing (feature engineering).\n",
        "  * Source of the data: The data is downloaded from https://drive.google.com/file/d/10ISx1yXxfE20nKq6UqquUAD5Egk3hyqi/view?usp=sharing and https://drive.google.com/file/d/1Y2z7FD5p5y31vkZwQQomXFRB0HutHyao/view. Since the officially released dataset (the prior link) is incomplete. The validation data is missing.\n",
        "  * Statistics: include basic descriptive statistics of the dataset like size, cross validation split, label distribution, etc.\n",
        "  * Data process: how do you munipulate the data, e.g., change the class labels, split the dataset to train/valid/test, refining the dataset.\n",
        "  * Illustration: printing results, plotting figures for illustration."
      ],
      "metadata": {
        "id": "2NbPHUTMbkD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from maskrcnn_benchmark.data.datasets import CocoDetection\n",
        "import os\n",
        "import os.path\n",
        "import math\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.utils.data as data\n",
        "from maskrcnn_benchmark.data.datasets.coco import COCODataset\n",
        "\n",
        "from maskrcnn_benchmark.structures.bounding_box import BoxList\n",
        "from maskrcnn_benchmark.structures.segmentation_mask import SegmentationMask\n",
        "from maskrcnn_benchmark.structures.keypoint import PersonKeypoints\n",
        "from maskrcnn_benchmark.config import cfg\n",
        "import pdb"
      ],
      "metadata": {
        "id": "iMqohLDqf43c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pil_loader(path, retry=5):\n",
        "    ri = 0\n",
        "    while ri < retry:\n",
        "        try:\n",
        "            with open(path, 'rb') as f:\n",
        "                img = Image.open(f)\n",
        "                return img.convert('RGB')\n",
        "        except:\n",
        "            ri += 1"
      ],
      "metadata": {
        "id": "pAG7IKR3f6dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rgb2id(color):\n",
        "    if isinstance(color, np.ndarray) and len(color.shape) == 3:\n",
        "        if color.dtype == np.uint8:\n",
        "            color = color.astype(np.int32)\n",
        "        return color[:, :, 0] + 256 * color[:, :, 1] + 256 * 256 * color[:, :, 2]\n",
        "    return int(color[0] + 256 * color[1] + 256 * 256 * color[2])"
      ],
      "metadata": {
        "id": "AZOowK1SgAzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CocoDetection(data.Dataset):\n",
        "    def __init__(self, root, annFile, transform=None, target_transform=None):\n",
        "        from pycocotools.coco import COCO\n",
        "        self.root = root\n",
        "        self.coco = COCO(annFile)\n",
        "        self.ids = list(self.coco.imgs.keys())\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __getitem__(self, index, return_meta=False):\n",
        "        coco = self.coco\n",
        "        img_id = self.ids[index]\n",
        "        if isinstance(img_id, str):\n",
        "            img_id = [img_id]\n",
        "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
        "        target = coco.loadAnns(ann_ids)\n",
        "\n",
        "        meta = coco.loadImgs(img_id)[0]\n",
        "        path = meta['file_name']\n",
        "        img = pil_loader(os.path.join(self.root, path))\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        if return_meta:\n",
        "            return img, target, meta\n",
        "        else:\n",
        "            return img, target, os.path.join(self.root, path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __repr__(self):\n",
        "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
        "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
        "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
        "        tmp = '    Transforms (if any): '\n",
        "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
        "        tmp = '    Target Transforms (if any): '\n",
        "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
        "        return fmt_str"
      ],
      "metadata": {
        "id": "Fm7CDx7bgEq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VqaCollator(object):\n",
        "    def __init__(self, size_divisible=0):\n",
        "        self.size_divisible = size_divisible\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        transposed_batch = list(zip(*batch))\n",
        "        images = transposed_batch[0]\n",
        "        targets = transposed_batch[1]\n",
        "        paths = transposed_batch[2]\n",
        "        return images, targets, paths\n",
        "\n",
        "def make_dataloader(root, annFile, transforms, **args):\n",
        "    print(root, annFile, \"root!!!!!!!!!!!!!!!\")\n",
        "    dataset = CocoDetection(root, annFile, transforms)\n",
        "    collate_batch = VqaCollator()\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        num_workers=8,\n",
        "        collate_fn=collate_batch\n",
        "    )\n",
        "\n",
        "    return data_loader"
      ],
      "metadata": {
        "id": "qGE3Dp5ugH5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##   Model\n",
        "The model includes the model definitation which usually is a class, model training, and other necessary parts.\n",
        "  * Model architecture: layer number/size/type, activation function, etc\n",
        "  * Training objectives: loss function, optimizer, weight of each loss term, etc\n",
        "  * Others: whether the model is pretrained, Monte Carlo simulation for uncertainty analysis, etc\n",
        "  * The code of model should have classes of the model, functions of model training, model validation, etc.\n",
        "  * If your model training is done outside of this notebook, please upload the trained model here and develop a function to load and test it."
      ],
      "metadata": {
        "id": "3muyDPFPbozY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from maskrcnn_benchmark.structures.image_list import to_image_list\n",
        "from maskrcnn_benchmark.structures.bounding_box import BoxList\n",
        "from maskrcnn_benchmark.structures.boxlist_ops import cat_boxlist\n",
        "\n",
        "from ..backbone import build_backbone\n",
        "from ..rpn import build_rpn\n",
        "from ..roi_heads import build_roi_heads\n",
        "\n",
        "from ..language_backbone import build_language_backbone\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "import random\n",
        "import timeit\n",
        "import pdb\n",
        "from copy import deepcopy"
      ],
      "metadata": {
        "id": "whh_oILMgUL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_word(input_ids, mask_token_id, vocabs, padding_token_id, greenlight_map):\n",
        "    output_label = deepcopy(input_ids)\n",
        "    for j in range(input_ids.size(0)):\n",
        "        for i in range(input_ids.size(1)):\n",
        "            prob = random.random()\n",
        "            ratio = 0.15\n",
        "            if greenlight_map is not None and greenlight_map[j,i] == -1:\n",
        "                output_label[j,i] = -100\n",
        "                continue\n",
        "\n",
        "            if (not input_ids[j,i] == padding_token_id) and prob < ratio:\n",
        "                prob /= ratio\n",
        "\n",
        "                if prob < 0.8:\n",
        "                    input_ids[j,i] = mask_token_id\n",
        "\n",
        "                elif prob < 0.9:\n",
        "                    input_ids[j,i] = random.choice(vocabs)\n",
        "\n",
        "            else:\n",
        "                output_label[j,i] = -100\n",
        "\n",
        "            if greenlight_map is not None and greenlight_map[j,i] != 1:\n",
        "                output_label[j,i] = -100\n",
        "    return input_ids, output_label"
      ],
      "metadata": {
        "id": "GXZ6OSiQhXdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GeneralizedVLRCNN(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super(GeneralizedVLRCNN, self).__init__()\n",
        "        self.cfg = cfg\n",
        "        self.backbone = build_backbone(cfg)\n",
        "\n",
        "        if cfg.MODEL.LANGUAGE_BACKBONE.TOKENIZER_TYPE == \"clip\":\n",
        "            from transformers import CLIPTokenizerFast\n",
        "            if cfg.MODEL.DYHEAD.FUSE_CONFIG.MLM_LOSS:\n",
        "                print(\"Reuse token 'ðŁĴĳ</w>' (token_id = 49404) for mask token!\")\n",
        "                self.tokenizer = CLIPTokenizerFast.from_pretrained(\"openai/clip-vit-base-patch32\",\n",
        "                                            from_slow=True, mask_token='ðŁĴĳ</w>')\n",
        "            else:\n",
        "                self.tokenizer = CLIPTokenizerFast.from_pretrained(\"openai/clip-vit-base-patch32\",\n",
        "                                            from_slow=True)\n",
        "        else:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(cfg.MODEL.LANGUAGE_BACKBONE.TOKENIZER_TYPE)\n",
        "        self.tokenizer_vocab = self.tokenizer.get_vocab()\n",
        "        self.tokenizer_vocab_ids = [item for key, item in self.tokenizer_vocab.items()]\n",
        "\n",
        "        self.language_backbone = build_language_backbone(cfg)\n",
        "\n",
        "        self.rpn = build_rpn(cfg)\n",
        "        self.roi_heads = build_roi_heads(cfg)\n",
        "        self.DEBUG = cfg.MODEL.DEBUG\n",
        "\n",
        "        self.freeze_backbone = cfg.MODEL.BACKBONE.FREEZE\n",
        "        self.freeze_fpn = cfg.MODEL.FPN.FREEZE\n",
        "        self.freeze_rpn = cfg.MODEL.RPN.FREEZE\n",
        "        self.add_linear_layer = cfg.MODEL.DYHEAD.FUSE_CONFIG.ADD_LINEAR_LAYER\n",
        "\n",
        "        self.force_boxes = cfg.MODEL.RPN.FORCE_BOXES\n",
        "\n",
        "        if cfg.MODEL.LINEAR_PROB:\n",
        "            assert cfg.MODEL.BACKBONE.FREEZE, \"For linear probing, backbone should be frozen!\"\n",
        "            if hasattr(self.backbone, 'fpn'):\n",
        "                assert cfg.MODEL.FPN.FREEZE, \"For linear probing, FPN should be frozen!\"\n",
        "        self.linear_prob = cfg.MODEL.LINEAR_PROB\n",
        "        self.freeze_cls_logits = cfg.MODEL.DYHEAD.FUSE_CONFIG.USE_DOT_PRODUCT_TOKEN_LOSS\n",
        "        if cfg.MODEL.DYHEAD.FUSE_CONFIG.USE_DOT_PRODUCT_TOKEN_LOSS:\n",
        "            if hasattr(self.rpn.head, 'cls_logits'):\n",
        "                for p in self.rpn.head.cls_logits.parameters():\n",
        "                    p.requires_grad = False\n",
        "\n",
        "        self.freeze_language_backbone = self.cfg.MODEL.LANGUAGE_BACKBONE.FREEZE\n",
        "        if self.cfg.MODEL.LANGUAGE_BACKBONE.FREEZE:\n",
        "            for p in self.language_backbone.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "        self.use_mlm_loss = cfg.MODEL.DYHEAD.FUSE_CONFIG.MLM_LOSS\n",
        "        self.mlm_loss_for_only_positives = cfg.MODEL.DYHEAD.FUSE_CONFIG.MLM_LOSS_FOR_ONLY_POSITIVES\n",
        "\n",
        "    def train(self, mode=True):\n",
        "        super(GeneralizedVLRCNN, self).train(mode)\n",
        "        if self.freeze_backbone:\n",
        "            self.backbone.body.eval()\n",
        "            for p in self.backbone.body.parameters():\n",
        "                p.requires_grad = False\n",
        "        if self.freeze_fpn:\n",
        "            self.backbone.fpn.eval()\n",
        "            for p in self.backbone.fpn.parameters():\n",
        "                p.requires_grad = False\n",
        "        if self.freeze_rpn:\n",
        "            if hasattr(self.rpn, 'head'):\n",
        "                self.rpn.head.eval()\n",
        "            for p in self.rpn.parameters():\n",
        "                p.requires_grad = False\n",
        "        if self.linear_prob:\n",
        "            if self.rpn is not None:\n",
        "                for key, value in self.rpn.named_parameters():\n",
        "                    if not ('bbox_pred' in key or 'cls_logits' in key or 'centerness' in key or 'cosine_scale' in key or 'dot_product_projection_text' in key or 'head.log_scale' in key or 'head.bias_lang' in key or 'head.bias0' in key):\n",
        "                        value.requires_grad = False\n",
        "            if self.roi_heads is not None:\n",
        "                for key, value in self.roi_heads.named_parameters():\n",
        "                    if not ('bbox_pred' in key or 'cls_logits' in key or 'centerness' in key or 'cosine_scale' in key or 'dot_product_projection_text' in key or 'head.log_scale' in key or 'head.bias_lang' in key or 'head.bias0' in key):\n",
        "                        value.requires_grad = False\n",
        "        if self.freeze_cls_logits:\n",
        "            if hasattr(self.rpn.head, 'cls_logits'):\n",
        "                self.rpn.head.cls_logits.eval()\n",
        "                for p in self.rpn.head.cls_logits.parameters():\n",
        "                    p.requires_grad = False\n",
        "        if self.add_linear_layer:\n",
        "            if self.rpn is not None:\n",
        "                for key, p in self.rpn.named_parameters():\n",
        "                    if 'tunable_linear' in key:\n",
        "                        p.requires_grad = True\n",
        "\n",
        "        if self.freeze_language_backbone:\n",
        "            self.language_backbone.eval()\n",
        "            for p in self.language_backbone.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "    def forward(self,\n",
        "        images,\n",
        "        targets=None,\n",
        "        captions=None,\n",
        "        positive_map=None,\n",
        "        greenlight_map=None):\n",
        "        if self.training and targets is None:\n",
        "            raise ValueError(\"In training mode, targets should be passed\")\n",
        "        images = to_image_list(images)\n",
        "        device = images.tensors.device\n",
        "\n",
        "        language_dict_features = {}\n",
        "        if captions is not None:\n",
        "\n",
        "            tokenized = self.tokenizer.batch_encode_plus(captions,\n",
        "                                    max_length=self.cfg.MODEL.LANGUAGE_BACKBONE.MAX_QUERY_LEN,\n",
        "                                    padding='max_length' if self.cfg.MODEL.LANGUAGE_BACKBONE.PAD_MAX else \"longest\",\n",
        "                                    return_special_tokens_mask=True,\n",
        "                                    return_tensors='pt',\n",
        "                                    truncation=True).to(device)\n",
        "            if self.use_mlm_loss:\n",
        "                if not self.mlm_loss_for_only_positives:\n",
        "                    greenlight_map = None\n",
        "                input_ids, mlm_labels = random_word(\n",
        "                    input_ids=tokenized.input_ids,\n",
        "                    mask_token_id=self.tokenizer.mask_token_id,\n",
        "                    vocabs=self.tokenizer_vocab_ids,\n",
        "                    padding_token_id=self.tokenizer.pad_token_id,\n",
        "                    greenlight_map=greenlight_map)\n",
        "            else:\n",
        "                input_ids = tokenized.input_ids\n",
        "                mlm_labels = None\n",
        "\n",
        "            tokenizer_input = {\"input_ids\": input_ids,\n",
        "                               \"attention_mask\": tokenized.attention_mask}\n",
        "\n",
        "            if self.cfg.MODEL.LANGUAGE_BACKBONE.FREEZE:\n",
        "                with torch.no_grad():\n",
        "                    language_dict_features = self.language_backbone(tokenizer_input)\n",
        "            else:\n",
        "                language_dict_features = self.language_backbone(tokenizer_input)\n",
        "\n",
        "            if self.cfg.DATASETS.ONE_HOT:\n",
        "                new_masks = torch.zeros_like(language_dict_features['masks'],\n",
        "                                             device=language_dict_features['masks'].device)\n",
        "                new_masks[:, :self.cfg.MODEL.DYHEAD.NUM_CLASSES] = 1\n",
        "                language_dict_features['masks'] = new_masks\n",
        "\n",
        "            if self.cfg.MODEL.LANGUAGE_BACKBONE.MASK_SPECIAL:\n",
        "                language_dict_features[\"masks\"] = 1 - tokenized.special_tokens_mask\n",
        "\n",
        "            language_dict_features[\"mlm_labels\"] = mlm_labels\n",
        "\n",
        "        swint_feature_c4 = None\n",
        "        if 'vl' in self.cfg.MODEL.SWINT.VERSION:\n",
        "            inputs = {\"img\": images.tensors, \"lang\": language_dict_features}\n",
        "            visual_features, language_dict_features, swint_feature_c4 = self.backbone(inputs)\n",
        "        else:\n",
        "            visual_features = self.backbone(images.tensors)\n",
        "\n",
        "        if targets:\n",
        "            targets = [target.to(device)\n",
        "                       for target in targets if target is not None]\n",
        "\n",
        "        if self.force_boxes:\n",
        "            proposals = []\n",
        "            for t in targets:\n",
        "                tb = t.copy_with_fields([\"labels\"])\n",
        "                tb.add_field(\"scores\", torch.ones(tb.bbox.shape[0], dtype=torch.bool, device=tb.bbox.device))\n",
        "                proposals.append(tb)\n",
        "            if self.cfg.MODEL.RPN.RETURN_FUSED_FEATURES:\n",
        "                _, proposal_losses, fused_visual_features = self.rpn(\n",
        "                    images, visual_features, targets, language_dict_features,\n",
        "                    positive_map, captions, swint_feature_c4)\n",
        "            elif self.training:\n",
        "                null_loss = 0\n",
        "                for key, param in self.rpn.named_parameters():\n",
        "                    null_loss += 0.0 * param.sum()\n",
        "                proposal_losses = {('rpn_null_loss', null_loss)}\n",
        "        else:\n",
        "            proposals, proposal_losses, fused_visual_features = self.rpn(images, visual_features, targets, language_dict_features, positive_map,\n",
        "                                              captions, swint_feature_c4)\n",
        "        if self.roi_heads:\n",
        "            if self.cfg.MODEL.ROI_MASK_HEAD.PREDICTOR.startswith(\"VL\"):\n",
        "                if self.training:\n",
        "                    assert len(targets) == 1 and len(targets[0]) == len(positive_map), \"shape match assert for mask head!!\"\n",
        "                    targets[0].add_field(\"positive_map\", positive_map)\n",
        "            if self.cfg.MODEL.RPN.RETURN_FUSED_FEATURES:\n",
        "                x, result, detector_losses = self.roi_heads(\n",
        "                    fused_visual_features, proposals, targets,\n",
        "                    language_dict_features=language_dict_features,\n",
        "                    positive_map_label_to_token=positive_map if not self.training else None\n",
        "                )\n",
        "            else:\n",
        "                x, result, detector_losses = self.roi_heads(\n",
        "                    visual_features, proposals, targets,\n",
        "                    language_dict_features=language_dict_features,\n",
        "                    positive_map_label_to_token=positive_map if not self.training else None\n",
        "                )\n",
        "        else:\n",
        "            x = visual_features\n",
        "            result = proposals\n",
        "            detector_losses = {}\n",
        "\n",
        "        if self.training:\n",
        "            losses = {}\n",
        "            losses.update(detector_losses)\n",
        "            losses.update(proposal_losses)\n",
        "            return losses\n",
        "\n",
        "        return result"
      ],
      "metadata": {
        "id": "4RA_WwBthjUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##   Training & Evaluation\n",
        "This paper is focusing on leveraging pretrained vision-language models (VLMs) for medical image understanding. Therefore, it focuses on how to utilize the prompt to adapt the pretrained VLMs to the new medical image domain. Therefore, this model can do zero-shot medical image understanding tasks without training. The evaluation code is shown as below:"
      ],
      "metadata": {
        "id": "zTgyC-BuOhDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(self):\n",
        "    \"\"\"\n",
        "    Run per image evaluation on given images and store results\n",
        "    (a list of dict) in self.eval_imgs.\n",
        "    \"\"\"\n",
        "\n",
        "    self.params.img_ids = list(np.unique(self.params.img_ids))\n",
        "\n",
        "    if self.params.use_cats:\n",
        "        cat_ids = self.params.cat_ids\n",
        "    else:\n",
        "        cat_ids = [-1]\n",
        "\n",
        "    self._prepare()\n",
        "\n",
        "    self.ious = {\n",
        "        (img_id, cat_id): self.compute_iou(img_id, cat_id) for img_id in self.params.img_ids for cat_id in cat_ids\n",
        "    }\n",
        "\n",
        "    # loop through images, area range, max detection number\n",
        "    self.eval_imgs = [\n",
        "        self.evaluate_img(img_id, cat_id, area_rng)\n",
        "        for cat_id in cat_ids\n",
        "        for area_rng in self.params.area_rng\n",
        "        for img_id in self.params.img_ids\n",
        "    ]\n",
        "\n",
        "def _get_gt_dt(self, img_id, cat_id):\n",
        "    \"\"\"Create gt, dt which are list of anns/dets. If use_cats is true\n",
        "    only anns/dets corresponding to tuple (img_id, cat_id) will be\n",
        "    used. Else, all anns/dets in image are used and cat_id is not used.\n",
        "    \"\"\"\n",
        "    if self.params.use_cats:\n",
        "        gt = self._gts[img_id, cat_id]\n",
        "        dt = self._dts[img_id, cat_id]\n",
        "    else:\n",
        "        gt = [_ann for _cat_id in self.params.cat_ids for _ann in self._gts[img_id, cat_id]]\n",
        "        dt = [_ann for _cat_id in self.params.cat_ids for _ann in self._dts[img_id, cat_id]]\n",
        "    return gt, dt\n",
        "\n",
        "def compute_iou(self, img_id, cat_id):\n",
        "    gt, dt = self._get_gt_dt(img_id, cat_id)\n",
        "\n",
        "    if len(gt) == 0 and len(dt) == 0:\n",
        "        return []\n",
        "\n",
        "    idx = np.argsort([-d[\"score\"] for d in dt], kind=\"mergesort\")\n",
        "    dt = [dt[i] for i in idx]\n",
        "\n",
        "    iscrowd = [int(False)] * len(gt)\n",
        "\n",
        "    if self.params.iou_type == \"segm\":\n",
        "        ann_type = \"segmentation\"\n",
        "    elif self.params.iou_type == \"bbox\":\n",
        "        ann_type = \"bbox\"\n",
        "    else:\n",
        "        raise ValueError(\"Unknown iou_type for iou computation.\")\n",
        "    gt = [g[ann_type] for g in gt]\n",
        "    dt = [d[ann_type] for d in dt]\n",
        "\n",
        "    ious = mask_util.iou(dt, gt, iscrowd)\n",
        "    return ious\n",
        "\n",
        "def evaluate_img(self, img_id, cat_id, area_rng):\n",
        "    \"\"\"Perform evaluation for single category and image.\"\"\"\n",
        "    gt, dt = self._get_gt_dt(img_id, cat_id)\n",
        "\n",
        "    if len(gt) == 0 and len(dt) == 0:\n",
        "        return None\n",
        "\n",
        "    for g in gt:\n",
        "        if g[\"ignore\"] or (g[\"area\"] < area_rng[0] or g[\"area\"] > area_rng[1]):\n",
        "            g[\"_ignore\"] = 1\n",
        "        else:\n",
        "            g[\"_ignore\"] = 0\n",
        "\n",
        "    gt_idx = np.argsort([g[\"_ignore\"] for g in gt], kind=\"mergesort\")\n",
        "    gt = [gt[i] for i in gt_idx]\n",
        "\n",
        "    dt_idx = np.argsort([-d[\"score\"] for d in dt], kind=\"mergesort\")\n",
        "    dt = [dt[i] for i in dt_idx]\n",
        "    ious = self.ious[img_id, cat_id][:, gt_idx] if len(self.ious[img_id, cat_id]) > 0 else self.ious[img_id, cat_id]\n",
        "\n",
        "    num_thrs = len(self.params.iou_thrs)\n",
        "    num_gt = len(gt)\n",
        "    num_dt = len(dt)\n",
        "\n",
        "    gt_m = np.zeros((num_thrs, num_gt))\n",
        "    dt_m = np.zeros((num_thrs, num_dt))\n",
        "\n",
        "    gt_ig = np.array([g[\"_ignore\"] for g in gt])\n",
        "    dt_ig = np.zeros((num_thrs, num_dt))\n",
        "\n",
        "    for iou_thr_idx, iou_thr in enumerate(self.params.iou_thrs):\n",
        "        if len(ious) == 0:\n",
        "            break\n",
        "\n",
        "        for dt_idx, _dt in enumerate(dt):\n",
        "            iou = min([iou_thr, 1 - 1e-10])\n",
        "\n",
        "            m = -1\n",
        "            for gt_idx, _ in enumerate(gt):\n",
        "                if gt_m[iou_thr_idx, gt_idx] > 0:\n",
        "                    continue\n",
        "                if m > -1 and gt_ig[m] == 0 and gt_ig[gt_idx] == 1:\n",
        "                    break\n",
        "                if ious[dt_idx, gt_idx] < iou:\n",
        "                    continue\n",
        "                iou = ious[dt_idx, gt_idx]\n",
        "                m = gt_idx\n",
        "\n",
        "            if m == -1:\n",
        "                continue\n",
        "\n",
        "\n",
        "            dt_ig[iou_thr_idx, dt_idx] = gt_ig[m]\n",
        "            dt_m[iou_thr_idx, dt_idx] = gt[m][\"id\"]\n",
        "            gt_m[iou_thr_idx, m] = _dt[\"id\"]\n",
        "\n",
        "\n",
        "    dt_ig_mask = [\n",
        "        d[\"area\"] < area_rng[0] or d[\"area\"] > area_rng[1] or d[\"category_id\"] in self.img_nel[d[\"image_id\"]]\n",
        "        for d in dt\n",
        "    ]\n",
        "    dt_ig_mask = np.array(dt_ig_mask).reshape((1, num_dt))\n",
        "    dt_ig_mask = np.repeat(dt_ig_mask, num_thrs, 0)\n",
        "    dt_ig = np.logical_or(dt_ig, np.logical_and(dt_m == 0, dt_ig_mask))\n",
        "    return {\n",
        "        \"image_id\": img_id,\n",
        "        \"category_id\": cat_id,\n",
        "        \"area_rng\": area_rng,\n",
        "        \"dt_ids\": [d[\"id\"] for d in dt],\n",
        "        \"gt_ids\": [g[\"id\"] for g in gt],\n",
        "        \"dt_matches\": dt_m,\n",
        "        \"gt_matches\": gt_m,\n",
        "        \"dt_scores\": [d[\"score\"] for d in dt],\n",
        "        \"gt_ignore\": gt_ig,\n",
        "        \"dt_ignore\": dt_ig,\n",
        "    }"
      ],
      "metadata": {
        "id": "LjW9bCkouv8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##   Evaluation Metrics Explanation\n",
        "The evaluation metric is mainly mAP and mIOU for polyp detection. mAP is the average precision which combines recall and precision for ranked retrieval results. mIOU means \"mean intersection over union\", which is a metric prevalently used in segmentation and detection. The intersection means the intersection region of the predicted mask and the ground truth mask, while the union means the union region of the predicted mask and the ground truth mask. Therefore, IOU is the value of intersection divided by the value of union, and mIOU is IOU taking the average over all classes."
      ],
      "metadata": {
        "id": "32AYoBdoQXo8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "In this section, you should finish training your model training or loading your trained model. That is a great experiment! You should share the results with others with necessary metrics and figures.\n",
        "\n",
        "Please test and report results for all experiments that you run with:\n",
        "\n",
        "*   specific numbers (accuracy, AUC, RMSE, etc)\n",
        "*   figures (loss shrinkage, outputs from GAN, annotation or label of sample pictures, etc)\n"
      ],
      "metadata": {
        "id": "9M9BJL13QD1v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the screenshot of using Masked Language Modeling to generate prompts automatically:\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1BQLTl8PpvzlQGB2W7O9blUhxD4MJr8YQ)\n",
        "\n",
        "These are some samples of the generated prompts:\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1PSmS0ADU4UGsQmhk-9eY8k_A9jw2m-PN)\n",
        "\n",
        "We can see that they are in the form of \"xxx color, xxx shape xxx in xxx\"."
      ],
      "metadata": {
        "id": "R9p8B9mo0bhT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the result of completely using auto-generated prompts by Masked Language Modeling (MLM):\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1E4E85GKLukBEWojvQ-sjNAbZARxhr6od)\n"
      ],
      "metadata": {
        "id": "AaHdHSQWzprO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So after the project draft deadline, I have made the OFA model work so that we could run the results for the hybrid strategy of generating the prompts.\n",
        "\n",
        "This is the screenshot of using hybrid method to generate prompts automatically:\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1KPPT0Wm-Nw07l5j7fCnCmsWa92UDy3A5)\n",
        "\n",
        "These are some samples of the generated prompts:\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=13qz2Wt5-OaqTlPojsecgyTeO2K0kEYN9)\n",
        "\n",
        "We can see that they are in the form of \"xxx bump in some cells\", where the prompts are less flexible and informative than masked image modeling (MLM) strategy."
      ],
      "metadata": {
        "id": "N-PxsIjCUGaG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the result of completely using auto-generated prompts by the hybrid method:\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1EfeypZIzP0GPcEF3OfU4sSCjU3lChdE8)\n"
      ],
      "metadata": {
        "id": "wpw9GaOMz4QD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When comparing with the results in the paper, we can see that the results are actually close to the results of the Kvasir dataset in Table 3, as shown in the red box region as follows:\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1PvQ-ljCO2SbdLW4unF-2JcP0oJVLGNOW)\n",
        "\n",
        "Also, the trend of MLM outperforms the hybrid method also conforms with the experimental results conducted by me, as from the above results we can see that MLM generally has better performance than hybrid method. On the average precision part the superiority is obvious although for the average recall the hybrid method performs slightly better. Nevertheless, the average recall metric is not reported in the paper."
      ],
      "metadata": {
        "id": "XYQQpzLBVL2U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion\n",
        "\n",
        "In this section,you should discuss your work and make future plan. The discussion should address the following questions:\n",
        "  * Explain why it is not reproducible if your results are kind negative.\n",
        "  * Describe “What was easy” and “What was difficult” during the reproduction.\n",
        "  * Make suggestions to the author or other reproducers on how to improve the reproducibility.\n",
        "\n",
        "\n",
        "\n",
        "* I think this paper is reproducible. As discussed above, we could achieve comparable results as the paper reports. Also, our observation from the results of generating prompts with MLM is better than hybrid method is also consistent with the results in the paper.\n",
        "\n",
        "* The coding part of this project relatively easy, because the authors have provided the official implementation on GitHub. However, the environment building part is very difficult for me, as there are many dependencies and requirements.\n",
        "\n",
        "* I would recommend authors could try not using some packages that has specific version requirements, which makes it hard for other researchers to install the environment and reproduce the results."
      ],
      "metadata": {
        "id": "qH75TNU71eRH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Public GitHub Repo\n",
        "\n",
        "https://github.com/zsh2000/CS598-DL4H"
      ],
      "metadata": {
        "id": "LOfzaWz2Kmod"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "1.   Qin et al. Medical Image Understanding with Pretrained Vision Language Models: A Comprehensive Study. ICLR 2023.\n",
        "\n"
      ],
      "metadata": {
        "id": "SHMI2chl9omn"
      }
    }
  ]
}